"""
LLM Q&A SYSTEM USING LANGGraph
========================================
This example shows how to create a simple question-answering system
that uses LangGraph to orchestrate an LLM workflow.
"""

# ============================================
# SECTION 1: IMPORT STATEMENTS
# ============================================

# Core LangGraph components:
# - StateGraph: Main class to create workflow graphs
# - START: Special marker indicating graph entry point  
# - END: Special marker indicating graph exit point
from langgraph.graph import StateGraph, START, END 

# TypedDict: Used to define strict structure for our state dictionary
# Ensures type safety and clear documentation of state fields
from typing import TypedDict

# AzureChatOpenAI: LangChain's integration with Azure OpenAI services
# Allows us to use GPT models via Azure's infrastructure
from langchain_openai import AzureChatOpenAI

# dotenv: Library to load environment variables from .env file
# Keeps sensitive information (API keys) out of source code
from dotenv import load_dotenv

# os: Standard Python library for operating system interactions
# Used here to access environment variables
import os

# ============================================
# SECTION 2: LOAD ENVIRONMENT VARIABLES
# ============================================

# Load variables from .env file located in project root
# This file should contain sensitive configuration like:
# AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_VERSION, OPENAI_API_KEY
# Using load_dotenv() prevents hardcoding credentials
load_dotenv()

# ============================================
# SECTION 3: INITIALIZE THE LLM (LANGUAGE MODEL)
# ============================================

"""
LLM Configuration Explanation:
- deployment_name: Name of the Azure deployment (resource identifier)
- model_name: Specific GPT model variant (gpt-4.1-mini is a smaller version)
- temperature: Controls randomness (0.1 = very deterministic, little creativity)
- max_tokens: Maximum length of generated response in tokens (~500 words)
- azure_endpoint: URL of Azure OpenAI service endpoint
- api_version: Version of Azure OpenAI API to use
- api_key: Authentication key for Azure OpenAI service
- azure_deployment: Which deployed model instance to use

The llm object becomes our interface to the AI model - think of it as
a smart assistant we can ask questions to programmatically.
"""
llm = AzureChatOpenAI(
    deployment_name="gpt-4.1-mini",  # Azure deployment resource name
    model_name="gpt-4.1-mini",       # Model variant from OpenAI
    temperature=0.1,                  # Low temperature = focused, consistent answers
    max_tokens=500,                   # Limit response length to ~500 tokens
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),  # Get endpoint from .env
    api_version=os.getenv("AZURE_OPENAI_VERSION"),      # Get API version from .env
    api_key=os.getenv("OPENAI_API_KEY"),                # Get API key from .env
    azure_deployment="gpt-4.1-mini"   # Deployment to use (same as deployment_name)
)

# ============================================
# SECTION 4: DEFINE STATE STRUCTURE
# ============================================

"""
State in LangGraph is a dictionary that flows through the workflow.
TypedDict provides type hints for better code clarity and error checking.

The state contains:
- question: User's input question (string)
- answer: LLM's generated response (string)

Think of the state as a "data package" that gets passed between nodes.
Each node can read from and write to this package.
"""
class llmstate(TypedDict):
    """
    llmstate: Typed dictionary defining our workflow's data structure.
    
    Attributes:
        question (str): The question to be answered by the LLM
        answer (str): The response generated by the LLM
    """
    question: str   # Input: What the user wants to know
    answer: str     # Output: What the AI responds with

# ============================================
# SECTION 5: NODE FUNCTION DEFINITION
# ============================================

"""
Nodes are the processing units in a LangGraph workflow.
Each node is a function that:
1. Receives the current state
2. Performs some computation or action
3. Returns an updated state

This particular node (llm_qa) handles the core functionality:
taking a question, asking the LLM, and storing the answer.
"""
def llm_qa(state: llmstate) -> llmstate:
    """
    Node function that uses an LLM to answer questions.
    
    Workflow within this function:
    1. Extract question from incoming state
    2. Format question into proper prompt
    3. Send prompt to LLM (Azure OpenAI)
    4. Extract response from LLM
    5. Update state with the answer
    6. Return updated state
    
    Parameters:
        state (llmstate): Dictionary containing 'question' key
    
    Returns:
        llmstate: Updated dictionary with 'answer' key populated
    """
    
    # STEP 1: Extract the question from the incoming state
    # The state dictionary arrives with a 'question' key
    # We retrieve its value to process
    question = state['question']
   
    # STEP 2: Create a prompt for the LLM
    # Simple string formatting - prepares the question for the AI
    # Note: There's a space missing after "question:" in the prompt
    prompt = f'Answer the following question{question}'
    
    # STEP 3: Invoke the LLM with our prompt
    # .invoke() sends the prompt to Azure OpenAI and returns a response object
    # .content extracts just the text response from the full response object
    answer = llm.invoke(prompt).content
   
    # STEP 4: Update the state dictionary with the answer
    # We add a new key 'answer' or update existing one with LLM's response
    state['answer'] = answer

    # STEP 5: Return the modified state
    # LangGraph will take this returned state and pass it to the next node
    # In this case, the next "node" is END (termination)
    return state

# ============================================
# SECTION 6: GRAPH CONSTRUCTION
# ============================================

"""
Graph Construction Steps:
1. Create a StateGraph object, specifying the state type
2. Add nodes (processing functions) to the graph
3. Add edges (connections) between nodes
4. Compile the graph into an executable workflow

Visual representation of this graph:
    START → [llm_qa node] → END
        ↓           ↓          ↓
    Input     Process with   Output
    State       LLM          State
"""

# Create a new StateGraph that works with our llmstate structure
# This initializes an empty workflow graph
graph = StateGraph(llmstate)

# Add our llm_qa function as a node to the graph
# Parameters:
# - 'llm_qa': Name/label for this node (can be any string)
# - llm_qa: The actual function to execute when this node runs
graph.add_node('llm_qa', llm_qa)

# ============================================
# SECTION 7: DEFINING WORKFLOW EDGES
# ============================================

"""
Edges define the flow of control between nodes.
They determine the order in which nodes execute.

In this simple linear workflow:
- START → llm_qa: When workflow starts, go immediately to llm_qa node
- llm_qa → END: After llm_qa finishes, end the workflow

START and END are special built-in nodes that mark beginning and termination.
"""

# Connect START node to our llm_qa node
# This means: "When the workflow begins, execute the llm_qa node"
graph.add_edge(START, 'llm_qa')

# Connect llm_qa node to END node  
# This means: "After llm_qa finishes, the workflow is complete"
graph.add_edge('llm_qa', END)

# ============================================
# SECTION 8: COMPILE THE GRAPH
# ============================================

"""
.compile() transforms our graph definition into an executable workflow.

Before compilation: Graph is just a blueprint/structure definition
After compilation: Graph is an executable object with .invoke() method

Think of it like:
- Before compile(): A recipe written on paper
- After compile(): A cooking robot ready to follow the recipe
"""
workflow = graph.compile()

# ============================================
# SECTION 9: EXECUTE THE WORKFLOW
# ============================================

# Create initial state with our question
# This dictionary must match the llmstate structure
# Contains only 'question' since 'answer' will be generated
initial_state = {'question': 'how far is the moon from earth'}

"""
.invoke() execution process:
1. Takes initial_state as input
2. Passes it to START node
3. START passes it to llm_qa node (based on edges)
4. llm_qa node executes: extracts question, calls LLM, updates state
5. Updated state passes to END node
6. END terminates workflow and returns final state
"""
final_state = workflow.invoke(initial_state)

# ============================================
# SECTION 10: OUTPUT RESULT
# ============================================

"""
final_state contains:
{
    'question': 'how far is the moon from earth',
    'answer': 'The Moon is approximately 384,400 kilometers...'
}

Printing shows the complete state after workflow execution.
"""
print(final_state)

"""
EXPECTED OUTPUT STRUCTURE:
{
    'question': 'how far is the moon from earth',
    'answer': 'Detailed answer from GPT-4 about moon distance...'
}
"""

# ============================================
# KEY CONCEPTS DEMONSTRATED:
# ============================================
"""
1. State Management: Using TypedDict to define data structure
2. Node Creation: Functions that process state and return updates
3. Graph Building: Connecting nodes with edges to create workflow
4. LLM Integration: Using LangChain's AzureChatOpenAI wrapper
5. Workflow Execution: .invoke() method runs the entire graph
6. Environment Safety: Keeping API keys in .env file, not in code

This is a minimal but complete LangGraph application!
"""